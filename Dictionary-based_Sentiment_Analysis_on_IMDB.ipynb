{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dictionary-based Sentiment Analysis on IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNgOmx0JSSheIRMnsuKEfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikaTina/Project_ML-SII/blob/main/Dictionary-based_Sentiment_Analysis_on_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQtSp62GZMj6"
      },
      "source": [
        "\n",
        "# **Import Common Libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-qSQhEZYIHH"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVjtQMRIZHcX"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVu0VhsBZxO7",
        "outputId": "c8490ec8-0662-4d7e-e673-792f37588546"
      },
      "source": [
        "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
        "                                  origin=URL,\n",
        "                                  untar=True,\n",
        "                                  cache_dir='.',\n",
        "                                  cache_subdir='')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8MVq0TSbN6e",
        "outputId": "4c3f5bd0-3252-4231-b400-0edd5bcb1ff0"
      },
      "source": [
        "# The shutil module offers a number of high-level \n",
        "# operations on files and collections of files.\n",
        "import os\n",
        "import shutil\n",
        "# Create main directory path (\"/aclImdb\")\n",
        "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "# Create sub directory path (\"/aclImdb/train\")\n",
        "train_dir = os.path.join(main_dir, 'train')\n",
        "# Create sub directory path (\"/aclImdb/test\")\n",
        "test_dir = os.path.join(main_dir, 'test')\n",
        "# Remove unsup folder since this is a supervised learning task\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)\n",
        "# View the final train and test folder\n",
        "print(os.listdir(train_dir))\n",
        "print(os.listdir(test_dir))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unsupBow.feat', 'pos', 'labeledBow.feat', 'neg', 'urls_unsup.txt', 'urls_neg.txt', 'urls_pos.txt']\n",
            "['pos', 'labeledBow.feat', 'neg', 'urls_neg.txt', 'urls_pos.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8r1U1qMdhS5"
      },
      "source": [
        "# **Load data from directories**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZZlr00UdpWU"
      },
      "source": [
        "dir_train_pos = os.listdir(train_dir + \"/pos\")\n",
        "dir_train_neg = os.listdir(train_dir + \"/neg\")\n",
        "dir_test_pos = os.listdir(test_dir + \"/pos\")\n",
        "dir_test_neg = os.listdir(test_dir + \"/neg\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpYr_MyKdxyF"
      },
      "source": [
        "data = []\n",
        "pos_train_dir = os.path.join(train_dir,'pos')\n",
        "neg_train_dir = os.path.join(train_dir,'neg')\n",
        "pos_test_dir = os.path.join(test_dir,'pos')\n",
        "neg_test_dir = os.path.join(test_dir,'neg')\n",
        "\n",
        "for element in dir_train_pos:\n",
        "  input_file = open(pos_train_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Positive\"])\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_train_neg:\n",
        "  input_file = open(neg_train_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Negative\"])\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_test_pos:\n",
        "  input_file = open(pos_test_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Positive\"])\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_test_neg:\n",
        "  input_file = open(neg_test_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Negative\"])\n",
        "  input_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZmkqMx7Ey0i"
      },
      "source": [
        "# **DataFrame Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXEesIzZetOq"
      },
      "source": [
        "df = pd.DataFrame(data, columns=['review', 'sentiment'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Iqsgz0yaey8S",
        "outputId": "71105bf2-f77d-4502-e032-568302c6813b"
      },
      "source": [
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've received this movie from a cousin in Norw...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I saw this movie, and the play, and I have to ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Well, What can I say, other than these people ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With various Bogdanoviches and Gazzaras scatte...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chase has created a true phenomenon with The S...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>A cheesy \"B\" crime thriller of the early '50, ...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>A total and absolute waste of time. Bad acting...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>This movie changed my life! Hogan's performanc...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>This demented left-wing wipe-out trivializes D...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>Star Trek V definitely earns the dubious disti...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      I've received this movie from a cousin in Norw...  Positive\n",
              "1      I saw this movie, and the play, and I have to ...  Positive\n",
              "2      Well, What can I say, other than these people ...  Positive\n",
              "3      With various Bogdanoviches and Gazzaras scatte...  Positive\n",
              "4      Chase has created a true phenomenon with The S...  Positive\n",
              "...                                                  ...       ...\n",
              "49995  A cheesy \"B\" crime thriller of the early '50, ...  Negative\n",
              "49996  A total and absolute waste of time. Bad acting...  Negative\n",
              "49997  This movie changed my life! Hogan's performanc...  Negative\n",
              "49998  This demented left-wing wipe-out trivializes D...  Negative\n",
              "49999  Star Trek V definitely earns the dubious disti...  Negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0W_nJKnfEbH"
      },
      "source": [
        "# **First method: pysentiment2**\n",
        "Pysentiment is a library used for dictionary-based sentiment analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7sHiQ73e-Yu",
        "outputId": "9bc5c322-603e-4728-f77f-f5efc4639955"
      },
      "source": [
        "pip install pysentiment2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysentiment2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/7c/596f3028260310d6206b7b88fe7d37fdb367913bfac9195912b27ab3cadb/pysentiment2-0.1.1-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pysentiment2) (1.1.5)\n",
            "Requirement already satisfied: nltk>=2.0 in /usr/local/lib/python3.7/dist-packages (from pysentiment2) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=2.0->pysentiment2) (1.15.0)\n",
            "Installing collected packages: pysentiment2\n",
            "Successfully installed pysentiment2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dTuFY3Bjvjb"
      },
      "source": [
        "import pysentiment2 as ps"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPteqXMj8aD"
      },
      "source": [
        "Two dictionaries are provided in the library: Harvard IV-4 and Loughran and McDonald Financial Sentiment Dictionaries, which are sentiment dictionaries for general and financial sentiment analysis. In this work the dictionary chosen is the general one, so Harvard IV-4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYVdrCInkJp9"
      },
      "source": [
        "hiv4 = ps.HIV4()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzsXN1SR8dZc"
      },
      "source": [
        "The following variables are used to examine the results of the sentiment analysis: the first one is used to compute the percentage of accuracy, while the others are used to compute precision, recall and f1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXft7gDkNuN"
      },
      "source": [
        "# variables initialization\n",
        "true_pos_ps=0     # number of positive reviews classified as positive\n",
        "true_neg_ps=0     # number of negative reviews classified as negative\n",
        "false_pos_ps=0    # number of negative reviews classified as positive\n",
        "false_neg_ps=0    # number of positive reviews classified as negative"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koWxxBYp5QUz"
      },
      "source": [
        "The score computed using this library has 4 components:\n",
        "\n",
        "* Negative: number of negative tokens\n",
        "* Positive: number of positive tokens\n",
        "* Polarity: this is computed as (Pos-Neg)/(Pos+Neg) where Pos and Neg are respectively the number of positive and negative tokens, and it is a negative number if the review is classified as negative, and a positive one if the review is classified as positive\n",
        "* Subjectivity: this is computed as (Pos+Neg)/count(*)\n",
        "\n",
        "In this work we only use the Polarity component of the score, classifying the review as positive if this component is greater than 0, as neutral if it is exactly equal to 0, and as negative if it is lower than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN26RjzCzyqd"
      },
      "source": [
        "for row in df.itertuples():\n",
        "  # tokenization of the review\n",
        "  tokens = hiv4.tokenize(row.review)\n",
        "  # score computation\n",
        "  score = hiv4.get_score(tokens)\n",
        "  # increment of the involved variables:\n",
        "  # if the review was labeled as positive and classified as positive increment true_pos_ps\n",
        "  if (score['Polarity']>0 and row.sentiment=='Positive'):\n",
        "    true_pos_ps=true_pos_ps+1\n",
        "  # if the review was labeled as negative and classified as negative increment true_neg_ps\n",
        "  elif (score['Polarity']<0 and row.sentiment=='Negative'):\n",
        "    true_neg_ps=true_neg_ps+1\n",
        "  # if the review was labeled as negative and classified as positive increment false_pos_ps\n",
        "  elif (score['Polarity']>0 and row.sentiment=='Negative'):\n",
        "    false_pos_ps=false_pos_ps+1\n",
        "  # if the review was labeled as positive and classified as negative increment false_neg_ps\n",
        "  elif (score['Polarity']<0 and row.sentiment=='Positive'):\n",
        "    false_neg_ps=false_neg_ps+1\n",
        "  # if the review has been classifed as neutral (polarity equal to 0) we assign it randomly to false_pos_ps or false_neg_ps (depending on the index row):\n",
        "  # if the index row is even increment false_pos_ps\n",
        "  elif (score['Polarity']==0) and row.Index%2==0:\n",
        "    false_pos_ps=false_pos_ps+1\n",
        "  # if the index row is odd increment false_neg_ps\n",
        "  else:\n",
        "    false_neg_ps=false_neg_ps+1\n",
        "  # print to check the status of the computation\n",
        "  if (row.Index%500==0):\n",
        "    print(row.Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1JQwP3MDeuk"
      },
      "source": [
        "## Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YKXCmPnCp_d",
        "outputId": "dad06ab3-cfb6-47ae-bd6c-87cde0a1cfe2"
      },
      "source": [
        "# ACCURACY\n",
        "accuracy_ps = (true_pos_ps+true_neg_ps)/(true_pos_ps+true_neg_ps+false_pos_ps+false_neg_ps)\n",
        "print('The accuracy percentage is ' + str(accuracy_ps*100) + '%')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy percentage is 61.46%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzvEDmcmCw-q",
        "outputId": "200a5246-b287-4acd-daee-2c3568ef2324"
      },
      "source": [
        "# PRECISION\n",
        "precision_ps = true_pos_ps/(true_pos_ps+false_pos_ps)\n",
        "print('The precision is ' + str(precision_ps))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision is 0.5895928266941055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ8VTLmnEgyB",
        "outputId": "3283a724-3969-4d85-9f5f-d03b5fff52c1"
      },
      "source": [
        "# RECALL\n",
        "recall_ps = true_pos_ps/(true_pos_ps+false_neg_ps)\n",
        "print('The recall is ' + str(recall_ps))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The recall is 0.7906564163217031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW74Qmc2Ew-y",
        "outputId": "6944221b-1e30-42dd-ca0e-fdd69e332b3f"
      },
      "source": [
        "# F1-SCORE\n",
        "f_score_ps=(2*precision_ps*recall_ps)/(precision_ps+recall_ps)\n",
        "print('The f1-score is ' + str(f_score_ps))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The f1-score is 0.675479959582351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tc5ZAOQ_cnr"
      },
      "source": [
        "# **Second method: NLTK and Vader**\n",
        "NLTK (Natural Language Tool Kit) is a library for NLP, while Vader (Valence Aware Dictionary e sEntiment Reasoner) is instead a rule-based model for\n",
        "sentiment analysis, tuned to sentiments from social media.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjKsL7lN8H0l",
        "outputId": "b38c18fa-e407-479b-f4f8-afcd459be150"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jtzK6OVHlad"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD23_8YiHpbK",
        "outputId": "0420975c-7284-4e3a-8c8f-ce42dd08e99a"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stJntPkBHuc3",
        "outputId": "d92fe442-c843-457b-a4b5-099c063d4dfb"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K424OCL_H2PF"
      },
      "source": [
        "# variables initialization\n",
        "true_pos_nltk=0     # number of positive reviews classified as positive\n",
        "true_neg_nltk=0     # number of negative reviews classified as negative\n",
        "false_pos_nltk=0    # number of negative reviews classified as positive\n",
        "false_neg_nltk=0    # number of positive reviews classified as negative"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXzWPdTHOiET"
      },
      "source": [
        "The score computed using this library has 4 components:\n",
        "\n",
        "* Positive: proportion of the text that falls in the positive category\n",
        "* Neutral: proportion of the text that falls in the neutral category\n",
        "* Negative: proportion of the text that falls in the negative category\n",
        "* Compound: metric that calculates the sum of the ratings (one for each word) and normalizes it between -1 (extremely negative) and +1 (extremely positive)\n",
        "\n",
        "In this work we only use the compound component of the score, classifying the review as positive if this component is greater than 0, as neutral if it is exactly equal to 0, and as negative if it is lower than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHVbWj3AHw5C"
      },
      "source": [
        "for row in df.itertuples():\n",
        "  # score computation\n",
        "  score = vader.polarity_scores(row.review)\n",
        "  # increment of the involved variables:\n",
        "  # if the review was labeled as positive and classified as positive increment true_pos_ps\n",
        "  if (score['compound']>0 and row.sentiment=='Positive'):\n",
        "    true_pos_nltk=true_pos_nltk+1\n",
        "  # if the review was labeled as negative and classified as negative increment true_neg_ps\n",
        "  elif (score['compound']<0 and row.sentiment=='Negative'):\n",
        "    true_neg_nltk=true_neg_nltk+1\n",
        "  # if the review was labeled as negative and classified as positive increment false_pos_ps\n",
        "  elif (score['compound']>0 and row.sentiment=='Negative'):\n",
        "    false_pos_nltk=false_pos_nltk+1\n",
        "  # if the review was labeled as positive and classified as negative increment false_neg_ps\n",
        "  elif (score['compound']<0 and row.sentiment=='Positive'):\n",
        "    false_neg_nltk=false_neg_nltk+1\n",
        "  # if the review has been classifed as neutral (compound equal to 0) we assign it randomly to false_pos_ps or false_neg_ps (depending on the index row):\n",
        "  # if the index row is even\n",
        "  elif (score['compound']==0) and row.Index%2==0:\n",
        "    false_pos_nltk=false_pos_nltk+1\n",
        "  # if the index row is odd\n",
        "  else:\n",
        "    false_neg_nltk=false_neg_nltk+1\n",
        "  # print to check the status of the computation\n",
        "  if row.Index%500==0:\n",
        "    print(row.Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW49cv-DNUlT"
      },
      "source": [
        "## Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwmsFjAoNVBn",
        "outputId": "da3a4306-5e42-4a3c-83d8-b952cd0c1933"
      },
      "source": [
        "# ACCURACY\n",
        "accuracy_nltk = (true_pos_nltk+true_neg_nltk)/(true_pos_nltk+true_neg_nltk+false_pos_nltk+false_neg_nltk)\n",
        "print('The accuracy percentage is ' + str(accuracy_nltk*100) + '%')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy percentage is 69.504%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29VeH6iFNr3V",
        "outputId": "0ace51a7-bcdd-4968-c487-1fb10f788237"
      },
      "source": [
        "# PRECISION\n",
        "precision_nltk = true_pos_nltk/(true_pos_nltk+false_pos_nltk)\n",
        "print('The precision is ' + str(precision_nltk))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision is 0.6475913646410513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qXlgpp6OB4i",
        "outputId": "e56e31c7-aa5f-4d5f-8aeb-6f7b2c4db384"
      },
      "source": [
        "# RECALL\n",
        "recall_nltk = true_pos_nltk/(true_pos_nltk+false_neg_nltk)\n",
        "print('The recall is ' + str(recall_nltk))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The recall is 0.8556226747209665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NERet1fCOO9G",
        "outputId": "b756694c-f0e9-4b1f-f03d-6e00d75fe574"
      },
      "source": [
        "# F1-SCORE\n",
        "f_score_nltk=(2*precision_nltk*recall_nltk)/(precision_nltk+recall_nltk)\n",
        "print('The f1-score is ' + str(f_score_nltk))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The f1-score is 0.737212188060113\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}