{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dictionary-based Sentiment Analysis on IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJJBKYqMxnsn9SO4AHe0TC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikaTina/Project_ML-SII/blob/main/Dictionary_based_Sentiment_Analysis_on_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQtSp62GZMj6"
      },
      "source": [
        "\n",
        "# **Import Common Libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-qSQhEZYIHH"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVjtQMRIZHcX"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVu0VhsBZxO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7a92ec-26bf-4db9-8aac-00bf12fdd623"
      },
      "source": [
        "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
        "                                  origin=URL,\n",
        "                                  untar=True,\n",
        "                                  cache_dir='.',\n",
        "                                  cache_subdir='')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8MVq0TSbN6e",
        "outputId": "db45dcbc-4425-4337-9f25-038583ddd68d"
      },
      "source": [
        "# The shutil module offers a number of high-level \n",
        "# operations on files and collections of files.\n",
        "import os\n",
        "import shutil\n",
        "# Create main directory path (\"/aclImdb\")\n",
        "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "# Create sub directory path (\"/aclImdb/train\")\n",
        "train_dir = os.path.join(main_dir, 'train')\n",
        "# Create sub directory path (\"/aclImdb/test\")\n",
        "test_dir = os.path.join(main_dir, 'test')\n",
        "# Remove unsup folder since this is a supervised learning task\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)\n",
        "# View the final train and test folder\n",
        "print(os.listdir(train_dir))\n",
        "print(os.listdir(test_dir))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['urls_unsup.txt', 'urls_neg.txt', 'neg', 'urls_pos.txt', 'pos', 'labeledBow.feat', 'unsupBow.feat']\n",
            "['urls_neg.txt', 'neg', 'urls_pos.txt', 'pos', 'labeledBow.feat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8r1U1qMdhS5"
      },
      "source": [
        "# **Load data from directories**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZZlr00UdpWU"
      },
      "source": [
        "dir_train_pos = os.listdir(train_dir + \"/pos\")\n",
        "dir_train_neg = os.listdir(train_dir + \"/neg\")\n",
        "dir_test_pos = os.listdir(test_dir + \"/pos\")\n",
        "dir_test_neg = os.listdir(test_dir + \"/neg\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpYr_MyKdxyF"
      },
      "source": [
        "data = []\n",
        "pos_train_dir = os.path.join(train_dir,'pos')\n",
        "neg_train_dir = os.path.join(train_dir,'neg')\n",
        "pos_test_dir = os.path.join(test_dir,'pos')\n",
        "neg_test_dir = os.path.join(test_dir,'neg')\n",
        "\n",
        "for element in dir_train_pos:\n",
        "  input_file = open(pos_train_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Positive\"])\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_train_neg:\n",
        "  input_file = open(neg_train_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Negative\"])\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_test_pos:\n",
        "  input_file = open(pos_test_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Positive\"])\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_test_neg:\n",
        "  input_file = open(neg_test_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  data.append([line, \"Negative\"])\n",
        "  input_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZmkqMx7Ey0i"
      },
      "source": [
        "# **DataFrame Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXEesIzZetOq"
      },
      "source": [
        "df = pd.DataFrame(data, columns=['review', 'sentiment'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Iqsgz0yaey8S",
        "outputId": "e63f788b-4a9a-44ce-da10-af2232d47665"
      },
      "source": [
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A deplorable social condition triggers off the...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I loved this movie! It's truly bizarre, extrem...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I would not have known about this film if not ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ok, so it borrows a little from \"It's a Wonder...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Despite what the title may imply, \"Pigs Is Pig...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>who reads these comments may think we may have...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>This film is one of the worst adaptations of P...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Supposedly, a movie about a magazine sending j...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>Tom &amp; Jerry are visiting Africa and disguise t...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>This has to be one of the all time greatest ho...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      A deplorable social condition triggers off the...  Positive\n",
              "1      I loved this movie! It's truly bizarre, extrem...  Positive\n",
              "2      I would not have known about this film if not ...  Positive\n",
              "3      Ok, so it borrows a little from \"It's a Wonder...  Positive\n",
              "4      Despite what the title may imply, \"Pigs Is Pig...  Positive\n",
              "...                                                  ...       ...\n",
              "49995  who reads these comments may think we may have...  Negative\n",
              "49996  This film is one of the worst adaptations of P...  Negative\n",
              "49997  Supposedly, a movie about a magazine sending j...  Negative\n",
              "49998  Tom & Jerry are visiting Africa and disguise t...  Negative\n",
              "49999  This has to be one of the all time greatest ho...  Negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0W_nJKnfEbH"
      },
      "source": [
        "# **First method: pysentiment2**\n",
        "Pysentiment is a library used for dictionary-based sentiment analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7sHiQ73e-Yu",
        "outputId": "ada95959-5d7c-4cb4-e358-a9da0770da60"
      },
      "source": [
        "!pip install pysentiment2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysentiment2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/7c/596f3028260310d6206b7b88fe7d37fdb367913bfac9195912b27ab3cadb/pysentiment2-0.1.1-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pysentiment2) (1.1.5)\n",
            "Requirement already satisfied: nltk>=2.0 in /usr/local/lib/python3.7/dist-packages (from pysentiment2) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pysentiment2) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=2.0->pysentiment2) (1.15.0)\n",
            "Installing collected packages: pysentiment2\n",
            "Successfully installed pysentiment2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dTuFY3Bjvjb"
      },
      "source": [
        "import pysentiment2 as ps"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPteqXMj8aD"
      },
      "source": [
        "Two dictionaries are provided in the library: Harvard IV-4 and Loughran and McDonald Financial Sentiment Dictionaries, which are sentiment dictionaries for general and financial sentiment analysis. In this work the dictionary chosen is the general one, so Harvard IV-4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYVdrCInkJp9"
      },
      "source": [
        "hiv4 = ps.HIV4()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzsXN1SR8dZc"
      },
      "source": [
        "The following variables are used to examine the results of the sentiment analysis: the first one is used to compute the percentage of accuracy, while the others are used to compute precision, recall and f1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXft7gDkNuN"
      },
      "source": [
        "# variables initialization\n",
        "true_pos_ps=0     # number of positive reviews classified as positive\n",
        "true_neg_ps=0     # number of negative reviews classified as negative\n",
        "false_pos_ps=0    # number of negative reviews classified as positive\n",
        "false_neg_ps=0    # number of positive reviews classified as negative"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koWxxBYp5QUz"
      },
      "source": [
        "The score computed using this library has 4 components:\n",
        "\n",
        "* Negative: number of negative tokens\n",
        "* Positive: number of positive tokens\n",
        "* Polarity: this is computed as (Pos-Neg)/(Pos+Neg) where Pos and Neg are respectively the number of positive and negative tokens, and it is used to classify the review\n",
        "* Subjectivity: this is computed as (Pos+Neg)/count(*)\n",
        "\n",
        "In this work we only use the 'Polarity' component of the score, and we have decided to classify the review as positive if this component is greater than or equal to 0, and as negative if it is lower than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN26RjzCzyqd"
      },
      "source": [
        "for row in df.itertuples():\n",
        "  # tokenization of the review\n",
        "  tokens = hiv4.tokenize(row.review)\n",
        "  # score computation\n",
        "  score = hiv4.get_score(tokens)\n",
        "  # increment of the involved variables:\n",
        "  # if the review was labeled as positive and classified as positive increment true_pos_ps\n",
        "  if ((score['Polarity']>0 or score['Polarity']==0) and row.sentiment=='Positive'):\n",
        "    true_pos_ps=true_pos_ps+1\n",
        "  # if the review was labeled as negative and classified as negative increment true_neg_ps\n",
        "  elif (score['Polarity']<0 and row.sentiment=='Negative'):\n",
        "    true_neg_ps=true_neg_ps+1\n",
        "  # if the review was labeled as negative and classified as positive increment false_pos_ps\n",
        "  elif ((score['Polarity']>0 or score['Polarity']==0) and row.sentiment=='Negative'):\n",
        "    false_pos_ps=false_pos_ps+1\n",
        "  # if the review was labeled as positive and classified as negative increment false_neg_ps\n",
        "  elif (score['Polarity']<0 and row.sentiment=='Positive'):\n",
        "    false_neg_ps=false_neg_ps+1\n",
        "  # print to check the status of the computation\n",
        "  if (row.Index%500==0):\n",
        "    print(row.Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1JQwP3MDeuk"
      },
      "source": [
        "## Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YKXCmPnCp_d",
        "outputId": "a96643ae-79b3-4051-edf7-bb020c2e1d3f"
      },
      "source": [
        "# ACCURACY\n",
        "accuracy_ps = (true_pos_ps+true_neg_ps)/(true_pos_ps+true_neg_ps+false_pos_ps+false_neg_ps)\n",
        "print('The accuracy percentage is ' + str(round(accuracy_ps*100,3)) + '%')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy percentage is 63.612%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzvEDmcmCw-q",
        "outputId": "847cffa1-d951-4e91-a242-cb8d1715bb14"
      },
      "source": [
        "# PRECISION\n",
        "precision_ps = true_pos_ps/(true_pos_ps+false_pos_ps)\n",
        "print('The precision is ' + str(round(precision_ps,3)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision is 0.596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ8VTLmnEgyB",
        "outputId": "64ef3720-d474-4831-d4f6-e6ad9525e77e"
      },
      "source": [
        "# RECALL\n",
        "recall_ps = true_pos_ps/(true_pos_ps+false_neg_ps)\n",
        "print('The recall is ' + str(round(recall_ps,3)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The recall is 0.845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW74Qmc2Ew-y",
        "outputId": "b54c5464-9651-4f9e-e731-38518918c5f1"
      },
      "source": [
        "# F1-SCORE\n",
        "f_score_ps=(2*precision_ps*recall_ps)/(precision_ps+recall_ps)\n",
        "print('The f1-score is ' + str(round(f_score_ps,3)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The f1-score is 0.699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tc5ZAOQ_cnr"
      },
      "source": [
        "# **Second method: NLTK and Vader**\n",
        "NLTK (Natural Language Tool Kit) is a library for NLP, while Vader (Valence Aware Dictionary e sEntiment Reasoner) is instead a rule-based method for\n",
        "sentiment analysis, tuned to sentiments from social media.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjKsL7lN8H0l",
        "outputId": "e03d272a-387f-45b2-cbd0-729f6a1448d0"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jtzK6OVHlad"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD23_8YiHpbK",
        "outputId": "7b762198-2fd8-47a7-9cd0-300448df7237"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stJntPkBHuc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7344f625-bd88-44b9-877f-67aa8315573a"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K424OCL_H2PF"
      },
      "source": [
        "# variables initialization\n",
        "true_pos_nltk=0     # number of positive reviews classified as positive\n",
        "true_neg_nltk=0     # number of negative reviews classified as negative\n",
        "false_pos_nltk=0    # number of negative reviews classified as positive\n",
        "false_neg_nltk=0    # number of positive reviews classified as negative"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXzWPdTHOiET"
      },
      "source": [
        "The score computed using this library has 4 components:\n",
        "\n",
        "* pos: proportion of the text that falls in the positive category\n",
        "* neu: proportion of the text that falls in the neutral category\n",
        "* neg: proportion of the text that falls in the negative category\n",
        "* compound: metric that calculates the sum of the ratings (one for each word) and normalizes it between -1 (extremely negative) and +1 (extremely positive)\n",
        "\n",
        "In this work we only use the 'compound' component of the score, and we have decided to classify the review as positive if this component is greater than or equal to 0, and as negative if it is lower than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHVbWj3AHw5C"
      },
      "source": [
        "for row in df.itertuples():\n",
        "  # score computation\n",
        "  score = vader.polarity_scores(row.review)\n",
        "  # increment of the involved variables:\n",
        "  # if the review was labeled as positive and classified as positive increment true_pos_ps\n",
        "  if ((score['compound']>0 or score['compound']==0) and row.sentiment=='Positive'):\n",
        "    true_pos_nltk=true_pos_nltk+1\n",
        "  # if the review was labeled as negative and classified as negative increment true_neg_ps\n",
        "  elif (score['compound']<0 and row.sentiment=='Negative'):\n",
        "    true_neg_nltk=true_neg_nltk+1\n",
        "  # if the review was labeled as negative and classified as positive increment false_pos_ps\n",
        "  elif ((score['compound']>0 or score['compound']==0) and row.sentiment=='Negative'):\n",
        "    false_pos_nltk=false_pos_nltk+1\n",
        "  # if the review was labeled as positive and classified as negative increment false_neg_ps\n",
        "  elif (score['compound']<0 and row.sentiment=='Positive'):\n",
        "    false_neg_nltk=false_neg_nltk+1\n",
        "  # print to check the status of the computation\n",
        "  if row.Index%500==0:\n",
        "    print(row.Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW49cv-DNUlT"
      },
      "source": [
        "## Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwmsFjAoNVBn",
        "outputId": "f95eeab5-8019-4858-e4b9-29ede6d8d787"
      },
      "source": [
        "# ACCURACY\n",
        "accuracy_nltk = (true_pos_nltk+true_neg_nltk)/(true_pos_nltk+true_neg_nltk+false_pos_nltk+false_neg_nltk)\n",
        "print('The accuracy percentage is ' + str(round(accuracy_nltk*100,3)) + ' %')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy percentage is 69.532 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29VeH6iFNr3V",
        "outputId": "1390b27d-682a-42ba-b2ff-2e7ba9676e42"
      },
      "source": [
        "# PRECISION\n",
        "precision_nltk = true_pos_nltk/(true_pos_nltk+false_pos_nltk)\n",
        "print('The precision is ' + str(round(precision_nltk,3)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The precision is 0.648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qXlgpp6OB4i",
        "outputId": "cac31135-2dfc-4238-f3af-895c60104d6f"
      },
      "source": [
        "# RECALL\n",
        "recall_nltk = true_pos_nltk/(true_pos_nltk+false_neg_nltk)\n",
        "print('The recall is ' + str(round(recall_nltk,3)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The recall is 0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NERet1fCOO9G",
        "outputId": "ccd4a3aa-eb5a-40e8-cbec-1a80da5e1860"
      },
      "source": [
        "# F1-SCORE\n",
        "f_score_nltk=(2*precision_nltk*recall_nltk)/(precision_nltk+recall_nltk)\n",
        "print('The f1-score is ' + str(round(f_score_nltk,3)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The f1-score is 0.738\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}