{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2 Sentiment on IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhqRZR5fMDyj3NeAfnFDiv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4te384bQOPd3"
      },
      "source": [
        "The code is based on https://medium.com/analytics-vidhya/gpt2-for-sentiment-analysis-38cd9832d5e9\n",
        "\n",
        "Modified to use the full IMDB of movie reviews instead of the 50 examples file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYux4bz3Oi5z"
      },
      "source": [
        "# **Import Libraries**\n",
        "Tensorflow 1.x is used because of dependencies needed with gpt_2_simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuHoDVj7hocN"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install gpt-2-simple \n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Bn6SWMT-3E"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8qESic1hovG"
      },
      "source": [
        "URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
        "                                  origin=URL,\n",
        "                                  untar=True,\n",
        "                                  cache_dir='.',\n",
        "                                  cache_subdir='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xf4YSa4UDzP"
      },
      "source": [
        "# **Preprocessing**\n",
        "The dataset chosed has tre principal folders: one contains files of positive reviews, one that contains files of negative reviews and a file of unlabeled reviews.\n",
        "We chose to exclude the unlabeled reviews and divide the labeled reviews in train and test sets, with no validation set. The objective is to hope that gpt2 learns to complete a sentence formatted as:\n",
        "\n",
        "// *review* || This review is *label* \n",
        "\n",
        "where *label* should be completed by gpt. \n",
        "\n",
        "The initial preprocessing of the files is inspired by https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URPmfFXhhpGW"
      },
      "source": [
        "# The shutil module offers a number of high-level \n",
        "# operations on files and collections of files.\n",
        "import os\n",
        "import shutil\n",
        "# Create main directory path (\"/aclImdb\")\n",
        "main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "# Create sub directory path (\"/aclImdb/train\")\n",
        "train_dir = os.path.join(main_dir, 'train')\n",
        "# Remove unsup folder since this is a supervised learning task\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)\n",
        "# View the final train folder\n",
        "print(os.listdir(train_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJPdwBF97RrT"
      },
      "source": [
        "dir_file_pos = os.listdir(train_dir+\"/pos\")\n",
        "print(\"Positivi: \") \n",
        "print(dir_file_pos)\n",
        "dir_file_neg = os.listdir(train_dir+\"/neg\")\n",
        "print(\"Negativi: \")\n",
        "print(dir_file_neg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ttqNcD48LdD"
      },
      "source": [
        "dataset = []\n",
        "pos_dir = os.path.join(train_dir,'pos')\n",
        "neg_dir = os.path.join(train_dir,'neg')\n",
        "\n",
        "for element in dir_file_pos:\n",
        "  input_file = open(pos_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  dataset.append(\"// \" + line + \" || This review is \" + \"Positive\" + \"\\n\")\n",
        "  input_file.close()\n",
        "\n",
        "for element in dir_file_neg:\n",
        "  input_file = open(neg_dir + \"/\"+ element, \"r\")\n",
        "  line = input_file.readline()\n",
        "  dataset.append(\"// \" + line + \" || This review is \" + \"Negative\" + \"\\n\")\n",
        "  input_file.close()\n",
        "\n",
        "#dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVDLG_bVD2-s"
      },
      "source": [
        "import random\n",
        "random.shuffle(dataset)\n",
        "#dataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOYCIiEhPYG2"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wCaEP1uEXeq"
      },
      "source": [
        "count = 0\n",
        "train = []\n",
        "while count < 20000:\n",
        "  train.append(dataset[count])\n",
        "  count = count + 1\n",
        "\n",
        "count = 20000\n",
        "test = []\n",
        "while count < 25000:\n",
        "    test.append(dataset[count])\n",
        "    count = count + 1\n",
        "#train"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqOm383HKmQP"
      },
      "source": [
        "#test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz9BJ-QGCwp7"
      },
      "source": [
        "file_train = open(\"train.txt\", \"w\")\n",
        "for element in train:\n",
        "  file_train.write(element)\n",
        "file_train.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RByqeXLiKn8d"
      },
      "source": [
        "#useless\n",
        "#file_test = open(\"test.txt\", \"w\")\n",
        "#for element in test:\n",
        "#  file_test.write(element)\n",
        "#file_test.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxkGsVXoYXEb"
      },
      "source": [
        "# **Downloading and Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ5jfNIrgGvF"
      },
      "source": [
        "#model_name = \"117M\"\n",
        "model_name = \"345M\" # The GPT-2 model we're using\n",
        "#model_name = \"774M\" #OOM, it was fun until it lasted\n",
        "gpt2.download_gpt2(model_name=model_name) # Download the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uObc8H6zuSc5"
      },
      "source": [
        "train_path = \"train.txt\"\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, #session\n",
        "              train_path, #path string of the file where the training set is saved\n",
        "              model_name=model_name, #used model \n",
        "              run_name='run1', # no idea i hope this works\n",
        "              checkpoint_dir='checkpoint', #same\n",
        "              steps=2)   # max number of training steps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym-jxwpjVbsT"
      },
      "source": [
        "# **Interaction with the model and results**\n",
        "Code reference that inspired this section:\n",
        "\n",
        "https://github.com/minimaxir/gpt-2-simple/blob/master/gpt_2_simple/gpt_2.py\n",
        "\n",
        "https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
        "\n",
        "https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
        "\n",
        "https://colab.research.google.com/drive/1ulO-Z0G6BdvQAZ83PJNXCS0ygtRtp46g\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oSuj1LtI0bC",
        "outputId": "a505165f-0d23-4485-cb53-fbb6f83c1983"
      },
      "source": [
        "#no idea what this does??? but has to be executed \n",
        "!git clone https://github.com/openai/gpt-2.git\n",
        "import os\n",
        "os.chdir(\"gpt-2/src/\")\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder\n",
        "os.chdir('../../')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 21.25 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLsnir3MCZ4P"
      },
      "source": [
        "samples = open(\"samples.txt\",\"w\")\n",
        "samples.write(\"Positive\"+\"\\n\")\n",
        "samples.write(\"Negative\"+\"\\n\")\n",
        "samples.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03fmCxUZ7P6H"
      },
      "source": [
        "correct = 0\n",
        "n_tests = 5000\n",
        "for element in test:\n",
        "  print(element + \"\\n\")\n",
        "  split = element.split(\"|| This review is \")\n",
        "  input = split[0].strip()\n",
        "  #print(input + \"\\n\")\n",
        "  input = input + \" ||  This review is \"\n",
        "  #print(input + \"\\n\")\n",
        "  expected_output = split[1].strip()\n",
        "  gpt2.generate(sess,\n",
        "             run_name='run1',\n",
        "             checkpoint_dir='checkpoint',\n",
        "             model_name=model_name,\n",
        "             model_dir='models',\n",
        "             sample_dir='samples',\n",
        "             return_as_list=False,\n",
        "             truncate=None,\n",
        "             destination_path=None,\n",
        "             sample_delim='\\n',\n",
        "             prefix=input,\n",
        "             seed=None,\n",
        "             nsamples=1,\n",
        "             batch_size=1,\n",
        "             length=8,\n",
        "             temperature=0.7,\n",
        "             top_k=0,\n",
        "             top_p=0.0,\n",
        "             include_prefix=True)\n",
        "\n",
        "  print(\"Expected: \" + expected_output + \", Answer: \") #+ str(out) + \"\\n\")\n",
        "  #if (str(out).casefold() == expected_output.casefold()):\n",
        "    #correct = correct + 1\n",
        "print(\"Correct \" + str(correct) + \" out of\" + str(n_tests) + \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSwf-KiZIYgP"
      },
      "source": [
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "#        while True:\n",
        "#            raw_text = input(\"Model prompt >>> \")\n",
        "#            while not raw_text:\n",
        "#                print('Prompt should not be empty!')\n",
        "#                raw_text = input(\"Model prompt >>> \")\n",
        "#            context_tokens = enc.encode(raw_text)\n",
        "#            generated = 0\n",
        "#            for _ in range(nsamples // batch_size):\n",
        "#                out = sess.run(output, feed_dict={\n",
        "#                    context: [context_tokens for _ in range(batch_size)]\n",
        "#                })[:, len(context_tokens):]\n",
        "#                for i in range(batch_size):\n",
        "#                    generated += 1\n",
        "#                    text = enc.decode(out[i])\n",
        "#                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "#                    print(text)\n",
        "#            print(\"=\" * 80)\n",
        "        n_tests = 5000\n",
        "        correct = 0\n",
        "        for element in test:\n",
        "          split = element.split(\"||  This review is\")\n",
        "          input = split[0].strip()\n",
        "          #print(input + \"\\n\")\n",
        "          input = input + \" ||  This review is \"\n",
        "          print(input + \"\\n\")\n",
        "          expected_output = split[1].strip()\n",
        "          context_tokens = enc.encode(input)\n",
        "          #due to a bug https://github.com/minimaxir/gpt-2-simple/issues/38\n",
        "          #long long reviews are excluded\n",
        "          try: \n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"Expected: \" + expected_output + \", Answer: \" + text + \"\\n\")\n",
        "                    if (text.casefold() == expected_output.casefold()):\n",
        "                      correct = correct + 1\n",
        "          except:\n",
        "            print(\"-------------- too long -------------\")\n",
        "            n_tests = n_tests - 1\n",
        "        print(\"Correct \" + str(correct) + \" out of\" + str(n_tests) + \"\\n\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icmzw6w8I3EI"
      },
      "source": [
        "#Test\n",
        "!ls\n",
        "interact_model(\n",
        "    'run1',\n",
        "    None,\n",
        "    1,\n",
        "    1,\n",
        "    2,\n",
        "    1,\n",
        "    0,\n",
        "    './checkpoint'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}